#Importing Modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np
import tensorflow as tf
 
import matplotlib.pyplot as plt
import seaborn as sns
 
from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,plot_roc_curve,f1_score
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import layers, losses, Sequential
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn import metrics
from tensorflow.keras.layers import Dense, Activation
# !pip install tensorflow
 
plt.style.use('seaborn-whitegrid') 
#Creating dataframes based on the dataset 
benign=pd.read_csv('data/5.benign.csv')
g_c=pd.read_csv('data/5.gafgyt.combo.csv')
g_j=pd.read_csv('data/5.gafgyt.junk.csv')
g_s=pd.read_csv('data/5.gafgyt.scan.csv')
g_t=pd.read_csv('data/5.gafgyt.tcp.csv')
g_u=pd.read_csv('data/5.gafgyt.udp.csv')
m_a=pd.read_csv('data/5.mirai.ack.csv')
m_sc=pd.read_csv('data/5.mirai.scan.csv')

m_sy=pd.read_csv('data/5.mirai.syn.csv')
m_u=pd.read_csv('data/5.mirai.udp.csv')
m_u_p=pd.read_csv('data/5.mirai.udpplain.csv')
 
#Sampling the dataset
benign=benign.sample(frac=0.25,replace=False)
g_c=g_c.sample(frac=0.25,replace=False)
g_j=g_j.sample(frac=0.5,replace=False)
g_s=g_s.sample(frac=0.5,replace=False)
g_t=g_t.sample(frac=0.15,replace=False)
g_u=g_u.sample(frac=0.15,replace=False)
m_a=m_a.sample(frac=0.25,replace=False)
m_sc=m_sc.sample(frac=0.15,replace=False)
m_sy=m_sy.sample(frac=0.25,replace=False)
m_u=m_u.sample(frac=0.1,replace=False)
m_u_p=m_u_p.sample(frac=0.27,replace=False)
 
#Creating column 'type' in each dataframe
benign['type']='benign'
m_u['type']='mirai_udp'
g_c['type']='gafgyt_combo'
g_j['type']='gafgyt_junk'
g_s['type']='gafgyt_scan'
g_t['type']='gafgyt_tcp'
g_u['type']='gafgyt_udp'
m_a['type']='mirai_ack'
m_sc['type']='mirai_scan'
m_sy['type']='mirai_syn'
m_u_p['type']='mirai_udpplain'
#Combining the different datasets to create the final dataset on which model is to be trained
data=pd.concat([benign,m_u,g_c,g_j,g_s,g_t,g_u,m_a,m_sc,m_sy,m_u_p],
               axis=0, sort=False, ignore_index=True)
 #how many instances of each class
data.groupby('type')['type'].count() 
#shuffle rows of dataframe 
sampler=np.random.permutation(len(data))
data=data.take(sampler)
data.head() 
#dummy encode labels, store separately
labels_full=pd.get_dummies(data['type'], prefix='type')
labels_full.head() 
#drop labels from training dataset
data_dup = data
data=data.drop(columns='type')
data.head() 
#standardize numerical columns
def standardize(df,col):
    df[col]= (df[col]-df[col].mean())/df[col].std()
data_st=data.copy()
for i in (data_st.iloc[:,:-1].columns):
    standardize (data_st,i)
 data_st.head()
 #training data for the neural net
train_data_st=data_st.values
train_data_st 

#labels for training
labels=labels_full.values
labels 
results = data_dup['type'].value_counts().to_dict()
dfResults = pd.DataFrame(results.items(),columns=['Type','Count'])
# Plotting a Pie Chart of the Results of the Shot
fig,ax = plt.subplots(figsize=(8,6))
plt.pie(dfResults['Count'],explode=(0,0,0.1,0,0,0,0.1,0,0,0,0.1),labels = ['mirai_syn',
'bashlite_tcp',
'mirai_udp',
'bashlite_udp',

'benign',
'bashlite_junk',
'bashlite_combo',
'mirai_udpplain',
'mirai_ack',
'bashlite_scan',
'mirai_scan'],shadow=True,textprops={'fontsize':15},autopct='%.0f%%')
plt.title('Distribution of various Infected and Benign Devices in the Dataset',fontsize=20,fontweight='bold')
plt.show()
# test/train split  25% test
x_train_st, x_test_st, y_train_st, y_test_st = train_test_split(
    train_data_st, labels, test_size=0.25, random_state=42)
 
#create and fit model
model = Sequential()
 
model.add(Dense(10, input_dim=train_data_st.shape[1], activation='relu'))
model.add(Dense(40, input_dim=train_data_st.shape[1], activation='relu'))
model.add(Dense(10, input_dim=train_data_st.shape[1], activation='relu'))
model.add(Dense(1, kernel_initializer='normal'))
model.add(Dense(labels.shape[1],activation='softmax'))
 
model.compile(loss='categorical_crossentropy', optimizer='adam')
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, 
                        patience=5, verbose=1, mode='auto')
 
model.fit(x_train_st,y_train_st,validation_data=(x_test_st,y_test_st),
          callbacks=[monitor],verbose=2,epochs=500) 
#Prediction of our model
pred_st = model.predict(x_test_st)
pred_st = np.argmax(pred_st,axis=1)
y_eval_st = np.argmax(y_test_st,axis=1)
 


#Finding the accuracy and recall of our model
score_st = metrics.accuracy_score(y_eval_st, pred_st)
score_st_tpr = metrics.recall_score(y_eval_st, pred_st, average="macro")
 
#Printing our model accuracy and recall
print("The Accuracy of our model: {}%:".format(round(score_st*100,2)))
print("The Recall of our model: {}%:".format(round(score_st_tpr*100,2))) 
# Plotting Confusion Matrix
fig,ax = plt.subplots(figsize=(7,7))
 
conf_matrix = confusion_matrix(y_eval_st, pred_st)
sns.set(font_scale=1.2)
sns.heatmap(conf_matrix/np.sum(conf_matrix),fmt='',cmap='Reds',ax=ax)
ax.tick_params(axis='both', which='major', labelsize=12) 
plt.title('Confusion Matrix of\nthe MLP',fontsize=18,fontweight='bold')
plt.show() 
from sklearn.ensemble import RandomForestClassifier
vanilla_rf = RandomForestClassifier(random_state=23)
 
#Fitting the model on the training data
vanilla_rf.fit(x_train_st,y_train_st)
 
#Calculatinf the AUC/ROC and accuracy on the test set
 
# y_proba_rf = vanilla_rf.predict_proba(x_test_st)[:,1]
y_pred_rf = vanilla_rf.predict(x_test_st)
y_pred_rf = np.argmax(y_pred_rf,axis=1)
y_eval_st = np.argmax(y_test_st,axis=1)

 acc_rf = np.round(metrics.accuracy_score(y_eval_st,y_pred_rf),2)
rec_rf = np.round(metrics.recall_score(y_eval_st,y_pred_rf,average="macro"),2)
 

print(f"The Accuracy on test set using Plain RF: {acc_rf*100}%")
print(f"The Recall of our test set using Plain RF: {rec_rf*100}%:") 
'''
# Assigning parameters to perform Grid-Search on
params = {'n_estimators' : [50,100,200,400],'criterion': ['gini', 'entropy'],'max_depth':[5,6,7,8,9,10]}
 
# Creating a Random Forest Model to perform Grid Search with
rf = RandomForestClassifier(random_state=23)
 
# Creating the GridSearch object 
clf = GridSearchCV(rf,params,cv=5,return_train_score=True,scoring='accuracy',n_jobs=-1)
 
# Fitting it on the training data
clf.fit(x_train_st,y_train_st)
'''
# Creating a Random Forest Model with the best hyper-parameters found through GridSearch
rfBest = RandomForestClassifier(criterion='entropy',max_depth=10,n_estimators=200, random_state=23) 
# Fitting on the training data
rfBest.fit(x_train_st,y_train_st) 
y_pred_rf = rfBest.predict(x_test_st)
y_pred_rf = np.argmax(y_pred_rf,axis=1)
y_eval_st = np.argmax(y_test_st,axis=1) 
acc_rf = np.round(metrics.accuracy_score(y_eval_st,y_pred_rf),2)
rec_rf = np.round(metrics.recall_score(y_eval_st,y_pred_rf,average="macro"),2)
 
print(f"The Accuracy on test set using Plain RF: {acc_rf*100}%")
print(f"The Recall of our test set using Plain RF: {rec_rf*100}%:")

 def load_nbaiot(filename):
    return np.loadtxt(
        os.path.join("data/", filename),
        delimiter=",",
        skiprows=1
    ) 
benign = load_nbaiot("5.benign.csv")
X_train = benign[:40000]
X_test0 = benign[40000:]
X_test1 = load_nbaiot("5.mirai.scan.csv")
X_test2 = load_nbaiot("5.mirai.ack.csv")
X_test3 = load_nbaiot("5.mirai.syn.csv")
X_test4 = load_nbaiot("5.mirai.udp.csv")
X_test5 = load_nbaiot("5.mirai.udpplain.csv") 

class Autoencoder(Model):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = Sequential([
            layers.Dense(115, activation="relu"),
            layers.Dense(86, activation="relu"),
            layers.Dense(57, activation="relu"),
            layers.Dense(37, activation="relu"),
            layers.Dense(28, activation="relu")
        ])
        self.decoder = Sequential([
            layers.Dense(37, activation="relu"),
            layers.Dense(57, activation="relu"),
            layers.Dense(86, activation="relu"),
            layers.Dense(115, activation="sigmoid")
        ])
    
    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded 
scaler = MinMaxScaler()
x = scaler.fit_transform(X_train)
 
ae = Autoencoder()
ae.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
monitor = EarlyStopping(
    monitor='val_loss',
    min_delta=1e-9,
    patience=5,
    verbose=1,
    mode='auto'
)
ae.fit(
    x=x,
    y=x,
    epochs=800,
    validation_split=0.3,
    shuffle=True,
    callbacks=[monitor]
)
 
training_loss = losses.mse(x_train_st, ae(x_train_st))
threshold = np.mean(training_loss)+np.std(training_loss) 
def predict(x, threshold=threshold, window_size=82):
    x = scaler.transform(x)
    predictions = losses.mse(x, ae(x)) > threshold
    # Majority voting over `window_size` predictions
    return np.array([np.mean(predictions[i-window_size:i]) > 0.5
                     for i in range(window_size, len(predictions)+1)])
 
def print_stats(data, outcome):
    print(f"Shape of data: {data.shape}")
    print(f"Detected anomalies: {np.mean(outcome)*100}%")
    print() 

test_data = [X_test0, X_test1, X_test2, X_test3, X_test4, X_test5]

 
for i, x in enumerate(test_data):
    print(i)
    outcome = predict(x)
    print_stats(x, outcome)

key = cv2.waitKey(0) #wait 1ms the loop will start again and we will process the next frame cv2.destroyAllWindows()

